{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "#name='path of file'\n",
    "#with_info= provides a tuple containing info about version, features and # samples of the data set\n",
    "#as_supervised = True, loads the data in a 2-tuple structure [input,target]\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting data\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation comes from train part, there is no specific part for validation\n",
    "#train contains 60,000 examples\n",
    "#test contains 10,000 examples\n",
    "\n",
    "#define the number of validation samples\n",
    "#but we don't know if this num is integer or float\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#therefore, we convert the variable into a given data type\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the num of test samples\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normally we'd like to scale the datain some way to make the result more numerically stable (inputs between 0-1)\n",
    "\n",
    "#take an MNIST iamge and its label\n",
    "#0-255\n",
    "#we want float\n",
    "#you can scale your data in other ways if you see fit.\n",
    "#make sure that func. takes image and label\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the whole train dataset and store it in variable\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what if every batch consist different procent of same element?\n",
    "#that is why, we need to shuffle the data as random as possible, in order to get the best results\n",
    "#therefore, we need buffer in order to put all data and shuffle in that buffer the all data at one time\n",
    "#if buffer size >= num_sample, shuffling will happen at once (uniformly)\n",
    "#if 1 < buffer size <= num_sample, we'll be optimizing the computational power of computer\n",
    "BUFFER_SIZE = 10000\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after that we can extract the exact train and validation sets\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "#.skip() method skips the given parameter and take the datas what is left from (all-given parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll use minibatch GD -> Sample size / batch size = element_in_one_batch\n",
    "#batch_size = 1 -> SGD\n",
    "#batch_size = # samples GD(single batch)\n",
    "#1<batch_size z # samples -> mini batch\n",
    "\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.batch(SIZE) -> put the given data into the given sizes of batches\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "#update the weights every batch(in every 100 element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to reshape validation_data into batch format too but when we forward propagate we need just 1 batch cause\n",
    "#we take all the datas at once, because we want exact values(not updating in every 100 element)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 22:28:01.462807: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "#validation data must have the same shape and obj. properties as the train and test data\n",
    "#remember that we did as_supervised -> loads the data in 2-Tuple structure [inputs,targets]\n",
    "#therefore, we need extract and convert the validation inputs/targets too\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "#.iter() = creates an obj. which can be iterated one element at a time\n",
    "#.next() = loads the next element of an iterable object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have 784 input, 10 output nodes, 2 hidden layer 50 nodes each\n",
    "#you can create different width hidden layers if it works better\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='sigmoid'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='sigmoid'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "#model.compile(optimizer=custom_optimizer, loss ='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss ='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#loss func for classifiers\n",
    "#there are 3 types of cross entropy -> binary, categorical, sparse_categorical\n",
    "#binary -> case in that we got binary encoding\n",
    "#categorical -> expects that you've one-hot encoded the targets\n",
    "#sparse_categorical -> applies one-hot encoding\n",
    "#increasing the learning rate from 0.0001 to 0.001 gives us better acc, can take a little bit more time or equal\n",
    "#using 'adam' as optimizer gives us bitter results rather than custom_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training part\n",
    "NUM_EPOCHS = 5\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)\n",
    "#if you decrease the batch_size acc increases but the time increases too\n",
    "#if you increase the hidden_layer_size, acc increases too but the time increases itself too\n",
    "#adding more layers can lower the acc and increase the time, not effektiv\n",
    "\n",
    "#these accs are validation accs. So it is possible that we overfitted the data\n",
    "#we have to test it too with forward propagating the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
